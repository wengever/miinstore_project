from KKT_Module.ksoc_global import kgl
from KKT_Module.Configs import SettingConfigs
from KKT_Module.SettingProcess.SettingProccess import SettingProc, ConnectDevice, ResetDevice
from KKT_Module.DataReceive.DataReciever import FeatureMapReceiver
from model import initial_model, predict_result, ConvLSTMNet
import sys
import time
from datetime import datetime
from iottalkpy import dan
import cv2

# ====== å»ºç«‹ DAN å®¢æˆ¶ç«¯ ======
client = dan.Client()

# ====== è¨­å®šè³‡è¨Š ======
device_model = 'Dummy_Device'
idf_list = [('DummySensor-I', [])]  # æ³¨æ„æ ¼å¼æ˜¯ tuple: (name, unit)
odf_list = []  # æ²’æœ‰æ§åˆ¶åŠŸèƒ½

def on_signal(cmd, df_list):
    print(f'[Signal] {cmd} on {df_list}')
    return True

def on_data(df_name, data):
    print(f'[ODF] {df_name} received: {data}')

try:
    client.register(
        url='https://miinstore.ncku.edu.tw/csm',
        on_signal=on_signal,
        on_data=on_data,
        idf_list=idf_list,
        odf_list=odf_list,
        name = 'wenggg',
        id_= '990b2d60-9ff8-4503-95dc-eb700c71a707',
        profile={
            "model": device_model,
            "is_sim": False,
        }
    )   
    print('[âœ”] è£ç½®è¨»å†ŠæˆåŠŸ')
except Exception as e:
    print('[âœ˜] è¨»å†Šå¤±æ•—:', e)
    sys.exit(1)

# === åˆå§‹åŒ–ç¡¬é«”é€£ç·š ===
def connect():
    connect = ConnectDevice()
    connect.startUp()
    reset = ResetDevice()
    reset.startUp()

# === è¨­å®šç¡¬é«”åƒæ•¸ ===
def startSetting():
    SettingConfigs.setScriptDir("K60168-Test-00256-008-v0.0.8-20230717_60cm")
    ksp = SettingProc()
    ksp.startUp(SettingConfigs)

# === æ‰‹å‹¢å½±ç‰‡æ’­æ”¾ ===
def play_gesture_video(gesture):
    gesture_videos = {
        "turn_up": "videos/turn_up.mp4",
        "turn_down": "videos/turn_down.mp4",
        "turn_left": "videos/turn_left.mp4",
        "turn_right": "videos/turn_right.mp4",
        "background": "videos/background.mp4",
    }

    video_path = gesture_videos.get(gesture)
    if video_path is None:
        print(f"âš ï¸ ç„¡å°æ‡‰å½±ç‰‡å¯æ’­æ”¾: {gesture}")
        return

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")
        return

    print(f"ğŸ¬ æ’­æ”¾æ‰‹å‹¢å½±ç‰‡: {gesture}")

    # å»ºç«‹ä¸¦è¨­å®šè¦–çª—ç‚ºç½®é ‚
    window_name = "Gesture Demo"
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.setWindowProperty(window_name, cv2.WND_PROP_TOPMOST, 1)

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        cv2.imshow("Gesture Demo", frame)
        # æŒ‰ q å¯ä¸­æ–·
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyWindow("Gesture Demo")

# === æ”¶é›†ä¸€ç­†æ‰‹å‹¢è³‡æ–™ï¼Œæ›´æ–°colorèˆ‡luminance ===
# ====== å…¨åŸŸè®Šæ•¸ ======
luminance = 99.0
color = 3.0
red = 255.0
green = 255.0
blue = 0.0

def update_rgb_by_color():
    global red, green, blue, color
    color_mod = color % 6.0
    if color_mod == 0.0:
        red, green, blue = 255.0, 0.0, 0.0
    elif color_mod == 1.0:
        red, green, blue = 0.0, 255.0, 0.0
    elif color_mod == 2.0:
        red, green, blue = 0.0, 0.0, 255.0
    elif color_mod == 3.0:
        red, green, blue = 255.0, 255.0, 0.0
    elif color_mod == 4.0:
        red, green, blue = 0.0, 255.0, 255.0
    elif color_mod == 5.0:
        red, green, blue = 255.0, 0.0, 255.0

def startLoop(R):
    global color, luminance, red, green, blue

    buffer_rdi = []
    buffer_phd = []

    print("âœ… é–‹å§‹æ”¶é›†æ‰‹å‹¢è³‡æ–™...è«‹ç¨å¾Œ...")

    while len(buffer_rdi) < 30:
        res = R.getResults()
        if res is None:
            continue
        buffer_rdi.append(res[0])
        buffer_phd.append(res[1])
        print(f"ğŸ“¡ æ”¶é›†ä¸­: {len(buffer_rdi)}/30 å¹€", end='\r')

    # é æ¸¬æ‰‹å‹¢
    gesture = predict_result(buffer_rdi, buffer_phd, model)
    print(f'\nâœ… åµæ¸¬åˆ°çš„æ‰‹å‹¢ç‚º: {gesture}')

    play_gesture_video(gesture)
    
    # æ ¹æ“šæ‰‹å‹¢æ›´æ–° luminance èˆ‡ color ä¸¦ç•¥é background
    if gesture == 'turn_up':
        luminance = min(luminance + 33.0, 99.0)
    elif gesture == 'turn_down':
        luminance = max(luminance - 33.0, 0.0)
    elif gesture == 'turn_right':
        color += 1.0
    elif gesture == 'turn_left':
        color -= 1.0

    # æ ¹æ“š color è¨ˆç®— RGB
    update_rgb_by_color()

    try:
        data_list = [luminance, red, green, blue]
        client.push('DummySensor-I', [data_list])
        print(datetime.now().isoformat(), f'â†’ å·²æ¨é€ï¼š{data_list}')
    except Exception as e:
        print(f'æ¨é€å¤±æ•—: {e}')
        return False

    return True



# ====== ä¸»ç¨‹å¼äº’å‹• ======
def main():
    global model
    kgl.setLib()
    connect()
    startSetting()
    model = initial_model()
    R = FeatureMapReceiver(chirps=32)
    R.trigger(chirps=32)
    time.sleep(0.5)

    print('# ======== è«‹è¼¸å…¥ start é–‹å§‹æ”¶é›†æ‰‹å‹¢ =========')

    try:
        while True:
            user_input = input("\nè«‹è¼¸å…¥ 'start' é–‹å§‹åµæ¸¬ï¼ˆCtrl+C é›¢é–‹ï¼‰ï¼š")
            if user_input.strip().lower() == 'start':
                startLoop(R)
            else:
                print("âŒ è¼¸å…¥éŒ¯èª¤ï¼Œè«‹è¼¸å…¥ 'start'ã€‚")

            time.sleep(0.5)  # é¿å…éå¿«çš„è¿´åœˆ
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ åµæ¸¬ä¸­æ–·ï¼ˆCtrl+Cï¼‰")

    finally:
        try:
            client.deregister()
            print("âœ… å·²è§£é™¤è£ç½®è¨»å†Š")
        except Exception as e:
            print("âš ï¸ è§£é™¤è¨»å†Šå¤±æ•—ï¼š", e)

if __name__ == '__main__':
    main()